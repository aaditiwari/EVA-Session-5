{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Norm BN L2 Amit Doda M6 Session 5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitdoda1983/EVA-Session-5/blob/master/Image_Norm_BN_L2_Amit_Doda_M6_Session_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxnnnZhE0LRk",
        "colab_type": "text"
      },
      "source": [
        "###Import Libraries and modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "3be6ff8f-9416-4bcc-85fc-db3f4ec47f52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 356,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7eef33cd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 356
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADoBJREFUeJzt3X2MXOV1x/HfyXq9jo1JvHHYboiL\nHeMEiGlMOjIgLKCiuA5CMiiKiRVFDiFxmuCktK4EdavGrWjlVgmRQynS0ri2I95CAsJ/0CR0FUGi\nwpbFMeYtvJlNY7PsYjZgQ4i9Xp/+sdfRBnaeWc/cmTu75/uRVjtzz71zj6792zszz8x9zN0FIJ53\nFd0AgGIQfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQU1r5M6mW5vP0KxG7hII5bd6U4f9kE1k\n3ZrCb2YrJG2W1CLpP9x9U2r9GZqls+2iWnYJIKHHuye8btVP+82sRdJNkj4h6QxJq83sjGofD0Bj\n1fKaf6mk5919j7sflnSHpJX5tAWg3moJ/8mSfjXm/t5s2e8xs7Vm1mtmvcM6VMPuAOSp7u/2u3uX\nu5fcvdSqtnrvDsAE1RL+fZLmjbn/wWwZgEmglvA/ImmRmS0ws+mSPi1pRz5tAai3qof63P2Ima2T\n9CONDvVtcfcnc+sMQF3VNM7v7vdJui+nXgA0EB/vBYIi/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q\nFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/\nEBThB4Ii/EBQhB8IivADQRF+IKiaZuk1sz5JByWNSDri7qU8mkJ+bFr6n7jl/XPruv9n/np+2drI\nzKPJbU9ZOJisz/yKJesv3zC9bG1n6c7ktvtH3kzWz75rfbJ+6l89nKw3g5rCn/kTd9+fw+MAaCCe\n9gNB1Rp+l/RjM3vUzNbm0RCAxqj1af8yd99nZidJut/MfuHuD45dIfujsFaSZmhmjbsDkJeazvzu\nvi/7PSjpHklLx1mny91L7l5qVVstuwOQo6rDb2azzGz2sduSlkt6Iq/GANRXLU/7OyTdY2bHHuc2\nd/9hLl0BqLuqw+/ueyR9LMdepqyW0xcl697Wmqy/dMF7k/W3zik/Jt3+nvR49U8/lh7vLtJ//WZ2\nsv4v/7YiWe8587aytReH30puu2ng4mT9Az/1ZH0yYKgPCIrwA0ERfiAowg8ERfiBoAg/EFQe3+oL\nb+TCjyfrN2y9KVn/cGv5r55OZcM+kqz//Y2fS9anvZkebjv3rnVla7P3HUlu27Y/PRQ4s7cnWZ8M\nOPMDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCM8+eg7ZmXkvVHfzsvWf9w60Ce7eRqff85yfqeN9KX\n/t668Ptla68fTY/Td3z7f5L1epr8X9itjDM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRl7o0b0TzR\n2v1su6hh+2sWQ1eem6wfWJG+vHbL7hOS9ce+cuNx93TM9fv/KFl/5IL0OP7Ia68n635u+au7930t\nuakWrH4svQLeoce7dcCH0nOXZzjzA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQFcf5zWyLpEslDbr7\n4mxZu6Q7Jc2X1Cdplbv/utLOoo7zV9Iy933J+sirQ8n6i7eVH6t/8vwtyW2X/vNXk/WTbiruO/U4\nfnmP82+V9PaJ0K+T1O3uiyR1Z/cBTCIVw+/uD0p6+6lnpaRt2e1tki7LuS8AdVbta/4Od+/Pbr8s\nqSOnfgA0SM1v+PnomwZl3zgws7Vm1mtmvcM6VOvuAOSk2vAPmFmnJGW/B8ut6O5d7l5y91Kr2qrc\nHYC8VRv+HZLWZLfXSLo3n3YANErF8JvZ7ZIekvQRM9trZldJ2iTpYjN7TtKfZvcBTCIVr9vv7qvL\nlBiwz8nI/ldr2n74wPSqt/3oZ55K1l+5uSX9AEdHqt43isUn/ICgCD8QFOEHgiL8QFCEHwiK8ANB\nMUX3FHD6tc+WrV15ZnpE9j9P6U7WL/jU1cn67DsfTtbRvDjzA0ERfiAowg8ERfiBoAg/EBThB4Ii\n/EBQjPNPAalpsl/98unJbf9vx1vJ+nXXb0/W/2bV5cm6//w9ZWvz/umh5LZq4PTxEXHmB4Ii/EBQ\nhB8IivADQRF+ICjCDwRF+IGgKk7RnSem6G4+Q58/N1m/9evfSNYXTJtR9b4/un1dsr7olv5k/cie\nvqr3PVXlPUU3gCmI8ANBEX4gKMIPBEX4gaAIPxAU4QeCqjjOb2ZbJF0qadDdF2fLNkr6oqRXstU2\nuPt9lXbGOP/k4+ctSdZP3LQ3Wb/9Qz+qet+n/eQLyfpH/qH8dQwkaeS5PVXve7LKe5x/q6QV4yz/\nlrsvyX4qBh9Ac6kYfnd/UNJQA3oB0EC1vOZfZ2a7zWyLmc3JrSMADVFt+G+WtFDSEkn9kr5ZbkUz\nW2tmvWbWO6xDVe4OQN6qCr+7D7j7iLsflXSLpKWJdbvcveTupVa1VdsngJxVFX4z6xxz93JJT+TT\nDoBGqXjpbjO7XdKFkuaa2V5JX5d0oZktkeSS+iR9qY49AqgDvs+PmrR0nJSsv3TFqWVrPdduTm77\nrgpPTD/z4vJk/fVlrybrUxHf5wdQEeEHgiL8QFCEHwiK8ANBEX4gKIb6UJjv7U1P0T3Tpifrv/HD\nyfqlX72m/GPf05PcdrJiqA9ARYQfCIrwA0ERfiAowg8ERfiBoAg/EFTF7/MjtqPL0pfufuFT6Sm6\nFy/pK1urNI5fyY1DZyXrM+/trenxpzrO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOP8U5yVFifr\nz34tPdZ+y3nbkvXzZ6S/U1+LQz6crD88tCD9AEf7c+xm6uHMDwRF+IGgCD8QFOEHgiL8QFCEHwiK\n8ANBVRznN7N5krZL6pDkkrrcfbOZtUu6U9J8SX2SVrn7r+vXalzTFpySrL9w5QfK1jZecUdy20+e\nsL+qnvKwYaCUrD+w+Zxkfc629HX/kTaRM/8RSevd/QxJ50i62szOkHSdpG53XySpO7sPYJKoGH53\n73f3ndntg5KelnSypJWSjn38a5uky+rVJID8HddrfjObL+ksST2SOtz92OcnX9boywIAk8SEw29m\nJ0j6gaRr3P3A2JqPTvg37qR/ZrbWzHrNrHdYh2pqFkB+JhR+M2vVaPBvdfe7s8UDZtaZ1TslDY63\nrbt3uXvJ3UutasujZwA5qBh+MzNJ35H0tLvfMKa0Q9Ka7PYaSffm3x6AepnIV3rPk/RZSY+b2a5s\n2QZJmyR9z8yukvRLSavq0+LkN23+Hybrr/9xZ7J+xT/+MFn/8/fenazX0/r+9HDcQ/9efjivfev/\nJredc5ShvHqqGH53/5mkcvN9X5RvOwAahU/4AUERfiAowg8ERfiBoAg/EBThB4Li0t0TNK3zD8rW\nhrbMSm775QUPJOurZw9U1VMe1u1blqzvvDk9Rffc7z+RrLcfZKy+WXHmB4Ii/EBQhB8IivADQRF+\nICjCDwRF+IGgwozzH/6z9GWiD//lULK+4dT7ytaWv/vNqnrKy8DIW2Vr5+9Yn9z2tL/7RbLe/lp6\nnP5osopmxpkfCIrwA0ERfiAowg8ERfiBoAg/EBThB4IKM87fd1n679yzZ95Vt33f9NrCZH3zA8uT\ndRspd+X0Uadd/2LZ2qKBnuS2I8kqpjLO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QlLl7egWzeZK2\nS+qQ5JK63H2zmW2U9EVJr2SrbnD38l96l3SitfvZxqzeQL30eLcO+FD6gyGZiXzI54ik9e6+08xm\nS3rUzO7Pat9y929U2yiA4lQMv7v3S+rPbh80s6clnVzvxgDU13G95jez+ZLOknTsM6PrzGy3mW0x\nszlltllrZr1m1jusQzU1CyA/Ew6/mZ0g6QeSrnH3A5JulrRQ0hKNPjP45njbuXuXu5fcvdSqthxa\nBpCHCYXfzFo1Gvxb3f1uSXL3AXcfcfejkm6RtLR+bQLIW8Xwm5lJ+o6kp939hjHLO8esdrmk9HSt\nAJrKRN7tP0/SZyU9bma7smUbJK02syUaHf7rk/SlunQIoC4m8m7/zySNN26YHNMH0Nz4hB8QFOEH\ngiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCoipfuznVnZq9I+uWY\nRXMl7W9YA8enWXtr1r4keqtWnr2d4u7vn8iKDQ3/O3Zu1uvupcIaSGjW3pq1L4neqlVUbzztB4Ii\n/EBQRYe/q+D9pzRrb83al0Rv1Sqkt0Jf8wMoTtFnfgAFKST8ZrbCzJ4xs+fN7LoieijHzPrM7HEz\n22VmvQX3ssXMBs3siTHL2s3sfjN7Lvs97jRpBfW20cz2Zcdul5ldUlBv88zsJ2b2lJk9aWZ/kS0v\n9Ngl+irkuDX8ab+ZtUh6VtLFkvZKekTSand/qqGNlGFmfZJK7l74mLCZnS/pDUnb3X1xtuxfJQ25\n+6bsD+ccd7+2SXrbKOmNomduziaU6Rw7s7SkyyR9TgUeu0Rfq1TAcSvizL9U0vPuvsfdD0u6Q9LK\nAvpoeu7+oKShty1eKWlbdnubRv/zNFyZ3pqCu/e7+87s9kFJx2aWLvTYJfoqRBHhP1nSr8bc36vm\nmvLbJf3YzB41s7VFNzOOjmzadEl6WVJHkc2Mo+LMzY30tpmlm+bYVTPjdd54w++dlrn7xyV9QtLV\n2dPbpuSjr9maabhmQjM3N8o4M0v/TpHHrtoZr/NWRPj3SZo35v4Hs2VNwd33Zb8HJd2j5pt9eODY\nJKnZ78GC+/mdZpq5ebyZpdUEx66ZZrwuIvyPSFpkZgvMbLqkT0vaUUAf72Bms7I3YmRmsyQtV/PN\nPrxD0prs9hpJ9xbYy+9plpmby80srYKPXdPNeO3uDf+RdIlG3/F/QdLfFtFDmb4+JOmx7OfJonuT\ndLtGnwYOa/S9kaskvU9St6TnJP23pPYm6u27kh6XtFujQessqLdlGn1Kv1vSruznkqKPXaKvQo4b\nn/ADguINPyAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQf0/sEWOix6VKakAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnDINRCMvyht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a1a78dfa-8852-4933-d0e6-43223e621574"
      },
      "source": [
        "print (X_train.mean(),X_train.std())"
      ],
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33.318421449829934 78.56748998339798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T6UK8Ii-Izh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "baa3b22d-e87c-420f-949a-60682608e94a"
      },
      "source": [
        "print (X_test.mean(),X_test.std())"
      ],
      "execution_count": 358,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33.791224489795916 79.17246322228644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvswb5Od0KQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X= X_train\n",
        "test_X = X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6-Xwzq36lWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdCAEaQzFjgD",
        "colab_type": "text"
      },
      "source": [
        "### Model with Image Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxVEhvwlu51b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "deefa172-7ceb-4b83-fcac-a893a65cd01a"
      },
      "source": [
        "# example of standardizing a image dataset\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# load dataset\n",
        "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
        "# reshape dataset to have a single channel\n",
        "width, height, channels = trainX.shape[1], trainX.shape[2], 1\n",
        "trainX = trainX.reshape((trainX.shape[0], width, height, channels))\n",
        "testX = testX.reshape((testX.shape[0], width, height, channels))\n",
        "# report pixel means and standard deviations\n",
        "print('Statistics train=%.3f (%.3f), test=%.3f (%.3f)' % (trainX.mean(), trainX.std(), testX.mean(), testX.std()))\n",
        "\n",
        "# create generator that centers pixel values\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "\n",
        "# calculate the mean on the training dataset\n",
        "datagen.fit(trainX)\n",
        "#print('Data Generator mean=%.3f, std=%.3f' % (datagen.mean, datagen.std))\n",
        "\n",
        "# demonstrate effect on a single batch of samples\n",
        "iterator = datagen.flow(trainX, trainy, batch_size=64)\n",
        "\n",
        "# get a batch\n",
        "batchX, batchy = iterator.next()\n",
        "\n",
        "# pixel stats in the batch\n",
        "print(batchX.shape, batchX.mean(), batchX.std())\n",
        "\n",
        "# demonstrate effect on entire training dataset\n",
        "iterator = datagen.flow(trainX, trainy, batch_size=len(trainX), shuffle=False)\n",
        "\n",
        "# get a batch\n",
        "batchX, batchy = iterator.next()\n",
        "\n",
        "# pixel stats in the batch\n",
        "print(batchX.shape, batchX.mean(), batchX.std())"
      ],
      "execution_count": 369,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics train=33.318 (78.567), test=33.791 (79.172)\n",
            "(64, 28, 28, 1) 0.041031793 1.0454384\n",
            "(60000, 28, 28, 1) -3.4560264e-07 0.9999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amW4ke4-04O3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterator1 = datagen.flow(testX, testy, batch_size=len(testX), shuffle=False)\n",
        "batch_testX, batch_testy = iterator1.next()\n",
        "                        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b33_Rj4-1McV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "855c91d2-da81-4911-9324-f86cfa027ab0"
      },
      "source": [
        "batch_testX.shape"
      ],
      "execution_count": 371,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 371
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbclA6OFHOW7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "01e8e67a-84a2-4bf4-f1da-a48e3cdc8e75"
      },
      "source": [
        "batch_testX.mean()"
      ],
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0060174568"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 372
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuAyWifLJMxU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9c177c9e-c6e3-49b3-840c-78163e92288d"
      },
      "source": [
        "batch_testX.std()"
      ],
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0077008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 373
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBSlvGhf2p9B",
        "colab_type": "text"
      },
      "source": [
        "Reshape the input to have the dimension for channel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = batchX\n",
        "X_test = batch_testX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdImE8gDJmEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=batchy\n",
        "y_test=batch_testy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vybkntk21Y-",
        "colab_type": "text"
      },
      "source": [
        "One hot encoding for output variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "e78d0434-37d7-491f-e264-e3fdc45a75b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 377,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 377
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Activation, MaxPooling2D,AveragePooling2D,Dropout,BatchNormalization\n",
        "import keras.callbacks\n",
        "from keras.callbacks import *\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "\n",
        "model.add(Convolution2D(8, (3, 3),activation='relu', input_shape=(28,28,1)))\n",
        "                               # Global Receptive field (3,3), output channel size =(26,26,8)\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, (3, 3),activation='relu'))\n",
        "                               # Global Receptive field (5,5), output channel size =(24,24,16)\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "                               # Global Receptive field (10,10), output channel size =(12,12,16)\n",
        "  \n",
        "model.add(Convolution2D(16, (3, 3),activation='relu'))\n",
        "                               # Global Receptive field (12,12), output channel size =(10,10,16)\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "                               # Global Receptive field (24,24), output channel size =(5,5,16)\n",
        "  \n",
        "model.add(Convolution2D(32, (3,3),activation='relu'))\n",
        "                               # Global Receptive field (26,26), output channel size =(3,3,32)\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model.add(Convolution2D(10, 1, activation='relu'))\n",
        "                               # Global Receptive field (26,26), output channel size =(3,3,10)\n",
        "model.add(BatchNormalization())\n",
        "                      \n",
        "model.add(AveragePooling2D(pool_size = (3, 3)))\n",
        "#model.add(Convolution2D(10, 3))\n",
        "                               # Global Receptive field (28,28), output channel size =(1,1,10)\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnPQPNDl8vKj",
        "colab_type": "text"
      },
      "source": [
        "### 8,706 Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "777e3710-5b18-49b0-e887-278457b71398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_123 (Conv2D)          (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "conv2d_124 (Conv2D)          (None, 24, 24, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling (None, 12, 12, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_125 (Conv2D)          (None, 10, 10, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_48 (MaxPooling (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_126 (Conv2D)          (None, 3, 3, 32)          4640      \n",
            "_________________________________________________________________\n",
            "conv2d_127 (Conv2D)          (None, 3, 3, 10)          330       \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 3, 3, 10)          40        \n",
            "_________________________________________________________________\n",
            "average_pooling2d_23 (Averag (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_23 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 8,706\n",
            "Trainable params: 8,622\n",
            "Non-trainable params: 84\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz9KfCuRf2V2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='model_best5.h5', monitor='val_loss', mode='auto', verbose = 1, save_best_only=True)\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_acc',factor=0.5, cooldown=0, patience=4, min_lr=0.5e-9,verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xWoKhPY9Of5",
        "colab_type": "code",
        "outputId": "d279e793-83f6-4e8d-fd27-2ee199a7ff07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3222
        }
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=64, nb_epoch=40, verbose=1,validation_data=(X_test, Y_test),callbacks=[checkpointer,lr_reducer])"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "60000/60000 [==============================] - 13s 213us/step - loss: 0.6021 - acc: 0.9032 - val_loss: 0.2621 - val_acc: 0.9660\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.26211, saving model to model_best5.h5\n",
            "Epoch 2/40\n",
            "60000/60000 [==============================] - 9s 146us/step - loss: 0.1782 - acc: 0.9729 - val_loss: 0.2109 - val_acc: 0.9687\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.26211 to 0.21094, saving model to model_best5.h5\n",
            "Epoch 3/40\n",
            "60000/60000 [==============================] - 8s 135us/step - loss: 0.1140 - acc: 0.9787 - val_loss: 0.1137 - val_acc: 0.9840\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.21094 to 0.11372, saving model to model_best5.h5\n",
            "Epoch 4/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0845 - acc: 0.9822 - val_loss: 0.1023 - val_acc: 0.9796\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.11372 to 0.10231, saving model to model_best5.h5\n",
            "Epoch 5/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0710 - acc: 0.9837 - val_loss: 0.0686 - val_acc: 0.9866\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.10231 to 0.06856, saving model to model_best5.h5\n",
            "Epoch 6/40\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.0608 - acc: 0.9849 - val_loss: 0.0733 - val_acc: 0.9832\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.06856\n",
            "Epoch 7/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0533 - acc: 0.9861 - val_loss: 0.0585 - val_acc: 0.9893\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.06856 to 0.05853, saving model to model_best5.h5\n",
            "Epoch 8/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0497 - acc: 0.9869 - val_loss: 0.0508 - val_acc: 0.9887\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.05853 to 0.05078, saving model to model_best5.h5\n",
            "Epoch 9/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0452 - acc: 0.9878 - val_loss: 0.0409 - val_acc: 0.9905\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.05078 to 0.04091, saving model to model_best5.h5\n",
            "Epoch 10/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0424 - acc: 0.9882 - val_loss: 0.0425 - val_acc: 0.9885\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.04091\n",
            "Epoch 11/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0419 - acc: 0.9884 - val_loss: 0.0398 - val_acc: 0.9906\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.04091 to 0.03978, saving model to model_best5.h5\n",
            "Epoch 12/40\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0385 - acc: 0.9889 - val_loss: 0.0377 - val_acc: 0.9909\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.03978 to 0.03768, saving model to model_best5.h5\n",
            "Epoch 13/40\n",
            "60000/60000 [==============================] - 8s 137us/step - loss: 0.0360 - acc: 0.9899 - val_loss: 0.0320 - val_acc: 0.9912\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.03768 to 0.03198, saving model to model_best5.h5\n",
            "Epoch 14/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0354 - acc: 0.9897 - val_loss: 0.0435 - val_acc: 0.9889\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.03198\n",
            "Epoch 15/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0336 - acc: 0.9903 - val_loss: 0.0403 - val_acc: 0.9886\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.03198\n",
            "Epoch 16/40\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0317 - acc: 0.9909 - val_loss: 0.0405 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.03198\n",
            "Epoch 17/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0311 - acc: 0.9911 - val_loss: 0.0425 - val_acc: 0.9884\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.03198\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 18/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0257 - acc: 0.9926 - val_loss: 0.0277 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.03198 to 0.02772, saving model to model_best5.h5\n",
            "Epoch 19/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0241 - acc: 0.9928 - val_loss: 0.0280 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.02772\n",
            "Epoch 20/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0238 - acc: 0.9931 - val_loss: 0.0257 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.02772 to 0.02571, saving model to model_best5.h5\n",
            "Epoch 21/40\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0252 - acc: 0.9924 - val_loss: 0.0239 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.02571 to 0.02391, saving model to model_best5.h5\n",
            "Epoch 22/40\n",
            "60000/60000 [==============================] - 9s 147us/step - loss: 0.0238 - acc: 0.9932 - val_loss: 0.0302 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.02391\n",
            "Epoch 23/40\n",
            "60000/60000 [==============================] - 8s 134us/step - loss: 0.0219 - acc: 0.9940 - val_loss: 0.0255 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.02391\n",
            "Epoch 24/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0213 - acc: 0.9940 - val_loss: 0.0236 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.02391 to 0.02359, saving model to model_best5.h5\n",
            "Epoch 25/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0221 - acc: 0.9933 - val_loss: 0.0224 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.02359 to 0.02239, saving model to model_best5.h5\n",
            "Epoch 26/40\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0214 - acc: 0.9940 - val_loss: 0.0268 - val_acc: 0.9921\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.02239\n",
            "Epoch 27/40\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0208 - acc: 0.9941 - val_loss: 0.0237 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.02239\n",
            "Epoch 28/40\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.0211 - acc: 0.9940 - val_loss: 0.0227 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.02239\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 29/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0191 - acc: 0.9950 - val_loss: 0.0215 - val_acc: 0.9938\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.02239 to 0.02149, saving model to model_best5.h5\n",
            "Epoch 30/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0186 - acc: 0.9947 - val_loss: 0.0218 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.02149\n",
            "Epoch 31/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0192 - acc: 0.9942 - val_loss: 0.0229 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.02149\n",
            "Epoch 32/40\n",
            "60000/60000 [==============================] - 9s 148us/step - loss: 0.0169 - acc: 0.9952 - val_loss: 0.0232 - val_acc: 0.9929\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.02149\n",
            "Epoch 33/40\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0170 - acc: 0.9951 - val_loss: 0.0237 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.02149\n",
            "Epoch 34/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0175 - acc: 0.9949 - val_loss: 0.0221 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.02149\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 35/40\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0166 - acc: 0.9954 - val_loss: 0.0209 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.02149 to 0.02095, saving model to model_best5.h5\n",
            "Epoch 36/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0160 - acc: 0.9957 - val_loss: 0.0218 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.02095\n",
            "Epoch 37/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0149 - acc: 0.9961 - val_loss: 0.0221 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.02095\n",
            "Epoch 38/40\n",
            "60000/60000 [==============================] - 8s 129us/step - loss: 0.0162 - acc: 0.9953 - val_loss: 0.0211 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.02095\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 39/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0152 - acc: 0.9955 - val_loss: 0.0214 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.02095\n",
            "Epoch 40/40\n",
            "60000/60000 [==============================] - 8s 130us/step - loss: 0.0150 - acc: 0.9957 - val_loss: 0.0210 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.02095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7f1254d400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLLM61RaXwt7",
        "colab_type": "text"
      },
      "source": [
        "## The validation accuracy achieved is 99.41\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "log9_0adiCiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=keras.models.load_model('model_best5.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "e8f1273f-e373-431e-d8a5-aa26adb16d51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.020946532106027007, 0.9941]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16oDs36CFoE0",
        "colab_type": "text"
      },
      "source": [
        "### Model with Image Normalization &  Relu after BN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEqSIQIwNF_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Activation, MaxPooling2D,AveragePooling2D,Dropout,BatchNormalization\n",
        "from keras import regularizers\n",
        "import keras.callbacks\n",
        "from keras.callbacks import *\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "\n",
        "model.add(Convolution2D(8, (3, 3),activation='relu', input_shape=(28,28,1)))\n",
        "                               # Global Receptive field (3,3), output channel size =(26,26,8)\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, (3, 3)))\n",
        "                               # Global Receptive field (5,5), output channel size =(24,24,16)\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "                               # Global Receptive field (10,10), output channel size =(12,12,16)\n",
        "  \n",
        "model.add(Convolution2D(16, (3, 3)))\n",
        "                               # Global Receptive field (12,12), output channel size =(10,10,16)\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "                               # Global Receptive field (24,24), output channel size =(5,5,16)\n",
        "  \n",
        "model.add(Convolution2D(32, (3,3),activation='relu'))\n",
        "                               # Global Receptive field (26,26), output channel size =(3,3,32)\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model.add(Convolution2D(10, 1))\n",
        "                               # Global Receptive field (26,26), output channel size =(3,3,10)\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "                      \n",
        "model.add(AveragePooling2D(pool_size = (3, 3)))\n",
        "#model.add(Convolution2D(10, 3))\n",
        "                               # Global Receptive field (28,28), output channel size =(1,1,10)\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qguIdAqtNGD_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "65296edb-7e78-47e4-86d6-9b4e6c4db0cc"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_168 (Conv2D)          (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "conv2d_169 (Conv2D)          (None, 24, 24, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_97 (Batc (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_86 (Activation)   (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_65 (Dropout)         (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_65 (MaxPooling (None, 12, 12, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_170 (Conv2D)          (None, 10, 10, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_98 (Batc (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_87 (Activation)   (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_66 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_66 (MaxPooling (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_171 (Conv2D)          (None, 3, 3, 32)          4640      \n",
            "_________________________________________________________________\n",
            "conv2d_172 (Conv2D)          (None, 3, 3, 10)          330       \n",
            "_________________________________________________________________\n",
            "batch_normalization_99 (Batc (None, 3, 3, 10)          40        \n",
            "_________________________________________________________________\n",
            "activation_88 (Activation)   (None, 3, 3, 10)          0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_32 (Averag (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_32 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_89 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 8,706\n",
            "Trainable params: 8,622\n",
            "Non-trainable params: 84\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cFQBgCMNGKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='model_best2.h5', monitor='val_loss', mode='auto', verbose = 1, save_best_only=True)\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_acc',factor=0.5, cooldown=0, patience=4, min_lr=0.5e-9,verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC5N2dysNGOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thr0_86DNGU_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3222
        },
        "outputId": "16f48531-b6ed-494d-f407-2784a355ce38"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=64, nb_epoch=40, verbose=1,validation_data=(X_test, Y_test),callbacks=[checkpointer,lr_reducer])"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "60000/60000 [==============================] - 17s 289us/step - loss: 0.6828 - acc: 0.9135 - val_loss: 0.4399 - val_acc: 0.9615\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.43993, saving model to model_best2.h5\n",
            "Epoch 2/40\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.2124 - acc: 0.9719 - val_loss: 0.2320 - val_acc: 0.9810\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.43993 to 0.23196, saving model to model_best2.h5\n",
            "Epoch 3/40\n",
            "60000/60000 [==============================] - 8s 142us/step - loss: 0.1327 - acc: 0.9784 - val_loss: 0.1703 - val_acc: 0.9820\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.23196 to 0.17033, saving model to model_best2.h5\n",
            "Epoch 4/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0986 - acc: 0.9819 - val_loss: 0.1212 - val_acc: 0.9850\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.17033 to 0.12118, saving model to model_best2.h5\n",
            "Epoch 5/40\n",
            "60000/60000 [==============================] - 10s 162us/step - loss: 0.0798 - acc: 0.9842 - val_loss: 0.0898 - val_acc: 0.9883\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.12118 to 0.08978, saving model to model_best2.h5\n",
            "Epoch 6/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0684 - acc: 0.9849 - val_loss: 0.0935 - val_acc: 0.9881\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.08978\n",
            "Epoch 7/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0570 - acc: 0.9870 - val_loss: 0.0678 - val_acc: 0.9910\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.08978 to 0.06782, saving model to model_best2.h5\n",
            "Epoch 8/40\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0536 - acc: 0.9873 - val_loss: 0.0644 - val_acc: 0.9889\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.06782 to 0.06438, saving model to model_best2.h5\n",
            "Epoch 9/40\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0475 - acc: 0.9879 - val_loss: 0.0549 - val_acc: 0.9897\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.06438 to 0.05492, saving model to model_best2.h5\n",
            "Epoch 10/40\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0456 - acc: 0.9886 - val_loss: 0.0491 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.05492 to 0.04908, saving model to model_best2.h5\n",
            "Epoch 11/40\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0408 - acc: 0.9897 - val_loss: 0.0524 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.04908\n",
            "Epoch 12/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0391 - acc: 0.9898 - val_loss: 0.0537 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.04908\n",
            "Epoch 13/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0370 - acc: 0.9908 - val_loss: 0.0459 - val_acc: 0.9898\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.04908 to 0.04589, saving model to model_best2.h5\n",
            "Epoch 14/40\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0359 - acc: 0.9904 - val_loss: 0.0422 - val_acc: 0.9908\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.04589 to 0.04220, saving model to model_best2.h5\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 15/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0304 - acc: 0.9919 - val_loss: 0.0381 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.04220 to 0.03810, saving model to model_best2.h5\n",
            "Epoch 16/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0291 - acc: 0.9923 - val_loss: 0.0365 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.03810 to 0.03651, saving model to model_best2.h5\n",
            "Epoch 17/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0286 - acc: 0.9927 - val_loss: 0.0353 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.03651 to 0.03533, saving model to model_best2.h5\n",
            "Epoch 18/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0282 - acc: 0.9925 - val_loss: 0.0358 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.03533\n",
            "Epoch 19/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0267 - acc: 0.9931 - val_loss: 0.0370 - val_acc: 0.9919\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.03533\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 20/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0244 - acc: 0.9938 - val_loss: 0.0325 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.03533 to 0.03251, saving model to model_best2.h5\n",
            "Epoch 21/40\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0234 - acc: 0.9939 - val_loss: 0.0305 - val_acc: 0.9936\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.03251 to 0.03054, saving model to model_best2.h5\n",
            "Epoch 22/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0232 - acc: 0.9940 - val_loss: 0.0305 - val_acc: 0.9939\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.03054 to 0.03048, saving model to model_best2.h5\n",
            "Epoch 23/40\n",
            "60000/60000 [==============================] - 10s 161us/step - loss: 0.0233 - acc: 0.9938 - val_loss: 0.0299 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.03048 to 0.02993, saving model to model_best2.h5\n",
            "Epoch 24/40\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0225 - acc: 0.9943 - val_loss: 0.0314 - val_acc: 0.9937\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.02993\n",
            "Epoch 25/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0232 - acc: 0.9939 - val_loss: 0.0297 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.02993 to 0.02968, saving model to model_best2.h5\n",
            "Epoch 26/40\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0222 - acc: 0.9940 - val_loss: 0.0298 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.02968\n",
            "Epoch 27/40\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0227 - acc: 0.9939 - val_loss: 0.0292 - val_acc: 0.9944\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.02968 to 0.02919, saving model to model_best2.h5\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 28/40\n",
            "60000/60000 [==============================] - 9s 153us/step - loss: 0.0210 - acc: 0.9943 - val_loss: 0.0287 - val_acc: 0.9943\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.02919 to 0.02866, saving model to model_best2.h5\n",
            "Epoch 29/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0203 - acc: 0.9949 - val_loss: 0.0265 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.02866 to 0.02646, saving model to model_best2.h5\n",
            "Epoch 30/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0209 - acc: 0.9945 - val_loss: 0.0270 - val_acc: 0.9944\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.02646\n",
            "Epoch 31/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0206 - acc: 0.9946 - val_loss: 0.0267 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.02646\n",
            "Epoch 32/40\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0206 - acc: 0.9946 - val_loss: 0.0274 - val_acc: 0.9946\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.02646\n",
            "Epoch 33/40\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.0194 - acc: 0.9951 - val_loss: 0.0256 - val_acc: 0.9955\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.02646 to 0.02557, saving model to model_best2.h5\n",
            "Epoch 34/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0193 - acc: 0.9950 - val_loss: 0.0269 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.02557\n",
            "Epoch 35/40\n",
            "60000/60000 [==============================] - 9s 146us/step - loss: 0.0194 - acc: 0.9949 - val_loss: 0.0265 - val_acc: 0.9955\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.02557\n",
            "Epoch 36/40\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.0196 - acc: 0.9950 - val_loss: 0.0274 - val_acc: 0.9940\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.02557\n",
            "Epoch 37/40\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.0198 - acc: 0.9947 - val_loss: 0.0255 - val_acc: 0.9950\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.02557 to 0.02546, saving model to model_best2.h5\n",
            "\n",
            "Epoch 00037: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 38/40\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0193 - acc: 0.9947 - val_loss: 0.0267 - val_acc: 0.9941\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.02546\n",
            "Epoch 39/40\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0196 - acc: 0.9948 - val_loss: 0.0268 - val_acc: 0.9942\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.02546\n",
            "Epoch 40/40\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0255 - val_acc: 0.9945\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.02546\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7efeeb8390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Te-BhnNaJ4N",
        "colab_type": "text"
      },
      "source": [
        "## The validation accuracy achieved is 99.55"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfIL3D_KNGYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0469651f-8bbb-496e-b590-a8d44463d0cd"
      },
      "source": [
        "model=keras.models.load_model('model_best2.h5')\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.025459150733426212, 0.995]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YwUkyQWUl_U",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "I observed that the model is giving beter results and converging faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5w-SUzQGH1H",
        "colab_type": "text"
      },
      "source": [
        "### Model with Image Normalization , Relu after BN & L2 Regularizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3e4VlrkSIxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Activation, MaxPooling2D,AveragePooling2D,Dropout,BatchNormalization\n",
        "from keras import regularizers\n",
        "import keras.callbacks\n",
        "from keras.callbacks import *\n",
        "model = Sequential()\n",
        "\n",
        " \n",
        "\n",
        "model.add(Convolution2D(8, (3, 3),activation='relu', input_shape=(28,28,1),kernel_regularizer=regularizers.l2(0.0000001)))\n",
        "                               # Global Receptive field (3,3), output channel size =(26,26,8)\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, (3, 3),kernel_regularizer=regularizers.l2(0.0000001)))\n",
        "                               # Global Receptive field (5,5), output channel size =(24,24,16)\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "                               # Global Receptive field (10,10), output channel size =(12,12,16)\n",
        "  \n",
        "model.add(Convolution2D(16, (3, 3),kernel_regularizer=regularizers.l2(0.0000001)))\n",
        "                               # Global Receptive field (12,12), output channel size =(10,10,16)\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "                               # Global Receptive field (24,24), output channel size =(5,5,16)\n",
        "  \n",
        "model.add(Convolution2D(32, (3,3),activation='relu',kernel_regularizer=regularizers.l2(0.0000001)))\n",
        "                               # Global Receptive field (26,26), output channel size =(3,3,32)\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model.add(Convolution2D(10, 1,kernel_regularizer=regularizers.l2(0.0000001)))\n",
        "                               # Global Receptive field (26,26), output channel size =(3,3,10)\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "                      \n",
        "model.add(AveragePooling2D(pool_size = (3, 3)))\n",
        "#model.add(Convolution2D(10, 3))\n",
        "                               # Global Receptive field (28,28), output channel size =(1,1,10)\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TpwKYo6SI8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "fc042c44-ff3d-43d5-9d49-f431c3ca9456"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 448,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_263 (Conv2D)          (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "conv2d_264 (Conv2D)          (None, 24, 24, 16)        1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_154 (Bat (None, 24, 24, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_162 (Activation)  (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_103 (Dropout)        (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_103 (MaxPoolin (None, 12, 12, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_265 (Conv2D)          (None, 10, 10, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_155 (Bat (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "activation_163 (Activation)  (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_104 (Dropout)        (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_104 (MaxPoolin (None, 5, 5, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_266 (Conv2D)          (None, 3, 3, 32)          4640      \n",
            "_________________________________________________________________\n",
            "conv2d_267 (Conv2D)          (None, 3, 3, 10)          330       \n",
            "_________________________________________________________________\n",
            "batch_normalization_156 (Bat (None, 3, 3, 10)          40        \n",
            "_________________________________________________________________\n",
            "activation_164 (Activation)  (None, 3, 3, 10)          0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_51 (Averag (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_51 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_165 (Activation)  (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 8,706\n",
            "Trainable params: 8,622\n",
            "Non-trainable params: 84\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFUaIqfRSJKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='model_best3.h5', monitor='val_acc', mode='auto', verbose = 1, save_best_only=True)\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_acc',factor=0.5, cooldown=0, patience=4, min_lr=0.5e-9,verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zySM7HhVSI3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZiijC5ANGdN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3259
        },
        "outputId": "e7eedb25-4d39-4d1a-c122-46e734ee158e"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=64, nb_epoch=40, verbose=1,validation_data=(X_test, Y_test),callbacks=[checkpointer,lr_reducer])"
      ],
      "execution_count": 451,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "60000/60000 [==============================] - 31s 512us/step - loss: 0.6998 - acc: 0.9097 - val_loss: 0.4399 - val_acc: 0.9676\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.96760, saving model to model_best3.h5\n",
            "Epoch 2/40\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.2175 - acc: 0.9733 - val_loss: 0.2528 - val_acc: 0.9773\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.96760 to 0.97730, saving model to model_best3.h5\n",
            "Epoch 3/40\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.1351 - acc: 0.9788 - val_loss: 0.1853 - val_acc: 0.9810\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.97730 to 0.98100, saving model to model_best3.h5\n",
            "Epoch 4/40\n",
            "60000/60000 [==============================] - 11s 182us/step - loss: 0.1008 - acc: 0.9822 - val_loss: 0.1312 - val_acc: 0.9856\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.98100 to 0.98560, saving model to model_best3.h5\n",
            "Epoch 5/40\n",
            "60000/60000 [==============================] - 10s 175us/step - loss: 0.0816 - acc: 0.9835 - val_loss: 0.1221 - val_acc: 0.9853\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.98560\n",
            "Epoch 6/40\n",
            "60000/60000 [==============================] - 11s 175us/step - loss: 0.0694 - acc: 0.9848 - val_loss: 0.0907 - val_acc: 0.9844\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.98560\n",
            "Epoch 7/40\n",
            "60000/60000 [==============================] - 10s 175us/step - loss: 0.0598 - acc: 0.9863 - val_loss: 0.0759 - val_acc: 0.9865\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.98560 to 0.98650, saving model to model_best3.h5\n",
            "Epoch 8/40\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0551 - acc: 0.9863 - val_loss: 0.1304 - val_acc: 0.9690\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.98650\n",
            "Epoch 9/40\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0505 - acc: 0.9878 - val_loss: 0.0700 - val_acc: 0.9860\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.98650\n",
            "Epoch 10/40\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0453 - acc: 0.9881 - val_loss: 0.0684 - val_acc: 0.9852\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.98650\n",
            "Epoch 11/40\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0437 - acc: 0.9890 - val_loss: 0.0592 - val_acc: 0.9892\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.98650 to 0.98920, saving model to model_best3.h5\n",
            "Epoch 12/40\n",
            "60000/60000 [==============================] - 11s 175us/step - loss: 0.0423 - acc: 0.9887 - val_loss: 0.0477 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.98920 to 0.98960, saving model to model_best3.h5\n",
            "Epoch 13/40\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0388 - acc: 0.9896 - val_loss: 0.0449 - val_acc: 0.9914\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.98960 to 0.99140, saving model to model_best3.h5\n",
            "Epoch 14/40\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0368 - acc: 0.9900 - val_loss: 0.0497 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.99140\n",
            "Epoch 15/40\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0357 - acc: 0.9905 - val_loss: 0.0483 - val_acc: 0.9893\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99140\n",
            "Epoch 16/40\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0340 - acc: 0.9908 - val_loss: 0.0483 - val_acc: 0.9896\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99140\n",
            "Epoch 17/40\n",
            "60000/60000 [==============================] - 10s 175us/step - loss: 0.0322 - acc: 0.9912 - val_loss: 0.0425 - val_acc: 0.9905\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99140\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "Epoch 18/40\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.0291 - acc: 0.9918 - val_loss: 0.0424 - val_acc: 0.9901\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99140\n",
            "Epoch 19/40\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0285 - acc: 0.9921 - val_loss: 0.0329 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.99140 to 0.99320, saving model to model_best3.h5\n",
            "Epoch 20/40\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0269 - acc: 0.9924 - val_loss: 0.0352 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99320\n",
            "Epoch 21/40\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.0258 - acc: 0.9930 - val_loss: 0.0342 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.99320\n",
            "Epoch 22/40\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0267 - acc: 0.9929 - val_loss: 0.0333 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.99320\n",
            "Epoch 23/40\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0248 - acc: 0.9934 - val_loss: 0.0352 - val_acc: 0.9924\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.99320\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "Epoch 24/40\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0233 - acc: 0.9940 - val_loss: 0.0333 - val_acc: 0.9919\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.99320\n",
            "Epoch 25/40\n",
            "60000/60000 [==============================] - 11s 189us/step - loss: 0.0238 - acc: 0.9934 - val_loss: 0.0318 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.99320\n",
            "Epoch 26/40\n",
            "60000/60000 [==============================] - 12s 201us/step - loss: 0.0220 - acc: 0.9938 - val_loss: 0.0332 - val_acc: 0.9916\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.99320\n",
            "Epoch 27/40\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.0219 - acc: 0.9942 - val_loss: 0.0310 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.99320 to 0.99340, saving model to model_best3.h5\n",
            "Epoch 28/40\n",
            "60000/60000 [==============================] - 11s 175us/step - loss: 0.0208 - acc: 0.9945 - val_loss: 0.0326 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.99340\n",
            "Epoch 29/40\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0223 - acc: 0.9937 - val_loss: 0.0327 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.99340\n",
            "Epoch 30/40\n",
            "60000/60000 [==============================] - 11s 186us/step - loss: 0.0204 - acc: 0.9948 - val_loss: 0.0323 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.99340\n",
            "Epoch 31/40\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.0206 - acc: 0.9945 - val_loss: 0.0321 - val_acc: 0.9927\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.99340\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "Epoch 32/40\n",
            "60000/60000 [==============================] - 11s 179us/step - loss: 0.0209 - acc: 0.9943 - val_loss: 0.0310 - val_acc: 0.9930\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.99340\n",
            "Epoch 33/40\n",
            "60000/60000 [==============================] - 11s 190us/step - loss: 0.0200 - acc: 0.9946 - val_loss: 0.0315 - val_acc: 0.9925\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.99340\n",
            "Epoch 34/40\n",
            "60000/60000 [==============================] - 10s 175us/step - loss: 0.0190 - acc: 0.9950 - val_loss: 0.0298 - val_acc: 0.9931\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.99340\n",
            "Epoch 35/40\n",
            "60000/60000 [==============================] - 11s 175us/step - loss: 0.0199 - acc: 0.9948 - val_loss: 0.0295 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.99340\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "Epoch 36/40\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0186 - acc: 0.9951 - val_loss: 0.0300 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.99340\n",
            "Epoch 37/40\n",
            "60000/60000 [==============================] - 10s 175us/step - loss: 0.0179 - acc: 0.9955 - val_loss: 0.0295 - val_acc: 0.9932\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.99340\n",
            "Epoch 38/40\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0283 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.99340\n",
            "Epoch 39/40\n",
            "60000/60000 [==============================] - 10s 175us/step - loss: 0.0182 - acc: 0.9956 - val_loss: 0.0288 - val_acc: 0.9933\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.99340\n",
            "\n",
            "Epoch 00039: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "Epoch 40/40\n",
            "60000/60000 [==============================] - 11s 191us/step - loss: 0.0185 - acc: 0.9950 - val_loss: 0.0287 - val_acc: 0.9934\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.99340\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7ee62d37b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 451
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFVIQi04S35l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "670fae2c-f489-4779-e958-fe94cb326e5a"
      },
      "source": [
        "model=keras.models.load_model('model_best3.h5')\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 452,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.030993917743116616, 0.9934]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwe0oALj58Md",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict=model.predict_classes(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDdjf0ZPU4tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incorrects = np.nonzero(model.predict_classes(X_test).reshape((-1,)) != y_test)\n",
        "incorrects=incorrects[0]\n",
        "incorrects=incorrects.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86746PMi8i21",
        "colab_type": "text"
      },
      "source": [
        "### 25 Misclassified Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbKIkD7GU48f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "d1902381-00fc-4608-9ebe-b7d5d475ff2e"
      },
      "source": [
        "fig=plt.figure(figsize=(20,20))\n",
        "rows=10\n",
        "columns = 10\n",
        "i=0\n",
        "for j in (incorrects):\n",
        "  if i <25:\n",
        "    fig.add_subplot(rows,columns,i+1)\n",
        "    plt.imshow(test_X[j])\n",
        "    i=i+1\n",
        "plt.show()"
      ],
      "execution_count": 459,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABH8AAAFcCAYAAABRKxK9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neOEqAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x1440 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyA6p-JvLii1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}